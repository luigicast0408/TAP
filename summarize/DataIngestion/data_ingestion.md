# Data Ingestion

## Il processo di acquisizione dei dati

Il processo di **data ingestion** puÃ² essere suddiviso in piÃ¹ fasi, che vanno dallâ€™identificazione delle fonti alla raccolta, trasformazione e infine caricamento dei dati nel sistema di destinazione.

1. **Identificazione delle fonti di dati**Il primo passo nellâ€™acquisizione dei dati consiste nellâ€™individuare e comprendere le **fonti di dati** rilevanti per le esigenze aziendali.Queste possono variare notevolmente a seconda del tipo di dato â€” **strutturato**, **semi-strutturato** o **non strutturato** â€” e del **dominio applicativo**.

   > ðŸ’¡ **Nota Bene:** I dati devono essere **accessibili**, sia dal punto di vista dei **permessi di utilizzo**, sia per quanto riguarda la **raggiungibilitÃ  tecnica** della fonte.
   >
2. **Raccolta dei dati**
   La **raccolta dei dati** Ã¨ il processo di acquisizione di informazioni da varie fonti per utilizzarle in successive **analisi, elaborazioni o archiviazioni**.
   Nel contesto della *data ingestion*, questa fase rappresenta lâ€™**acquisizione iniziale dei dati grezzi**, che saranno poi trasformati, convalidati e caricati in un sistema di gestione dei dati, come un **data lake** o un **data warehouse**.

   Per raccogliere i dati vengono utilizzate diverse strategie, in base alla natura della fonte e ai requisiti aziendali.
3. **Trasformazione e convalida dei dati**Dopo la raccolta, i dati grezzi devono essere **trasformati e convalidati** per garantirne lâ€™affidabilitÃ  e la coerenza.In questa fase si applicano operazioni come:

   - **Pulizia dei dati** (rimozione di duplicati e gestione dei valori mancanti);
   - **Normalizzazione** e **formattazione**;
   - **Verifica della qualitÃ ** e **conformitÃ  ai modelli di business**.
4. **Caricamento nel sistema di destinazione**
   I dati cosÃ¬ trasformati vengono infine **caricati** nel sistema di destinazione â€” ad esempio un **data warehouse**, un **data lake** o una piattaforma di **analytics** â€” dove potranno essere analizzati, combinati e utilizzati per generare valore informativo.
5. **Monitoraggio e aggiornamento continuo**
   Una volta implementato il flusso di acquisizione, Ã¨ essenziale **monitorare** le prestazioni e la qualitÃ  dei dati nel tempo.
   Il monitoraggio consente di individuare **errori, ritardi o anomalie** nel processo di ingestion e di adattare il sistema a **nuove sorgenti** o a **variazioni nei volumi dei dati**.

## Che cos'Ã¨ il Data Ingestion?

Lâ€™**acquisizione dei dati** (*data ingestion*) Ã¨ il processo mediante il quale i dati vengono **raccolti e importati** da una o piÃ¹ sorgenti per essere **utilizzati immediatamente** o **archiviati** allâ€™interno di un sistema di destinazione, come un database o un data lake.

Il termine *ingestion* deriva dal verbo inglese *to ingest*, che significa â€œassorbireâ€ o â€œintrodurreâ€: in questo contesto indica lâ€™**atto di trasferire e integrare i dati** allâ€™interno di un sistema, affinchÃ© possano essere analizzati o elaborati successivamente.

## Una storia di fusioni e acquisizioni

### La nascita di una nuova azienda nel campo dei dati

Câ€™era una volta unâ€™azienda chiamata **Talend**, fondata con lâ€™obiettivo di aiutare le organizzazioni a **raccogliere, trasformare e gestire i dati** provenienti da diversi ecosistemi.

Talend si Ã¨ distinta per il suo approccio **open source**, grazie al lancio di **Talend Open Studio**, una piattaforma che ha permesso agli utenti di **creare e distribuire flussi di integrazione dei dati** in modo semplice e flessibile.
Nel tempo, lâ€™azienda ha ampliato la propria offerta con soluzioni dedicate alla **qualitÃ  dei dati**, allâ€™**integrazione cloud** e alla **data governance**, consolidando cosÃ¬ la propria posizione nel settore.

### Crescita e acquisizioni per conquistare il mercato delle PMI

> Il mese scorso **Stitch** Ã¨ entrata a far parte di **Talend**.
> Talend Ã¨ unâ€™azienda globale di software open source specializzata nellâ€™integrazione di **Big Data** e **cloud**, la cui missione Ã¨ *â€œrendere i vostri dati migliori, piÃ¹ affidabili e piÃ¹ accessibili per generare valore aziendaleâ€*.
> Una visione perfettamente in linea con quella di Stitch: *â€œispirare e dare potere alle persone basate sui datiâ€*.

Grazie a questa fusione, Talend offre oggi una **piattaforma ETL SaaS** completa e senza attriti, integrando le potenzialitÃ  di Stitch.
Ãˆ possibile **estrarre dati da oltre 140 fonti popolari** e trasferirli nel proprio **data warehouse** o **database** in pochi minuti, senza bisogno di scrivere una sola riga di codice.

### Lâ€™acquisizione da parte di una societÃ  di Business Intelligence

Successivamente, **Talend** Ã¨ stata acquisita da **Qlik**, una delle principali aziende nel settore della **Business Intelligence (BI)** e dellâ€™**analisi dei dati**.

Lâ€™unione tra le due realtÃ  ha dato origine a unâ€™entitÃ  con una **posizione di leadership consolidata** in numerose categorie di mercato.
Per **sette anni consecutivi**, **Gartner** ha riconosciuto **Talend** come *leader* nel proprio **Magic Quadrant for Data Integration Tools** e per **cinque anni consecutivi** nel **Magic Quadrant for Data Quality Solutions**.

Parallelamente, **Qlik** Ã¨ stata classificata come *leader* nel **Magic Quadrant for Analytics and Business Intelligence Platforms** per **tredici anni di seguito**, confermando la propria eccellenza nel campo dellâ€™analisi e della visualizzazione dei dati.
Inoltre, **IDC MarketScape** ha nominato Qlik come *leader* nel report **â€œUS Business Intelligence and Analytics Platforms 2022 Vendor Assessmentâ€*, rafforzando ulteriormente la reputazione dellâ€™azienda nel panorama globale della BI.

## Data Ingestion vs Data Integration

Lâ€™**acquisizione dei dati** (*data ingestion*) Ã¨ un concetto simile, ma distinto, da quello di **integrazione dei dati** (*data integration*).
Mentre la *data integration* ha lâ€™obiettivo di **combinare piÃ¹ sorgenti di dati** in un sistema unificato e coerente â€” spesso **allâ€™interno dello stesso ambiente aziendale** â€” la *data ingestion* si concentra principalmente sul **trasferimento dei dati** da fonti esterne verso un sistema di destinazione.

In altre parole, lâ€™integrazione dei dati punta alla **fusione logica e strutturale** delle informazioni, mentre lâ€™acquisizione dei dati riguarda il **processo iniziale di raccolta e importazione**, che puÃ² coinvolgere **siti web, applicazioni SaaS** o **database esterni**.

## Metodi di raccolta dei dati

- **Batch ingestion** â†’ comporta la raccolta e lâ€™elaborazione dei dati a intervalli regolari (ad esempio ogni ora o ogni giorno).
  I dati vengono aggregati in grandi set e caricati in blocco, consentendo unâ€™elaborazione efficiente ma non in tempo reale.
- **Streaming ingestion** â†’ consiste nellâ€™acquisizione continua di dati in **tempo reale**, dove le informazioni vengono raccolte non appena generate.
  I dati fluiscono costantemente dalla sorgente alla destinazione, permettendo **analisi immediate** e **aggiornamenti dinamici**.
- **Micro-batch ingestion** â†’ rappresenta un approccio intermedio, in cui i dati vengono raccolti in piccoli blocchi a intervalli molto brevi, combinando lâ€™efficienza del batch con la reattivitÃ  dello streaming.

## Validazione dei dati

- **Pulizia dei dati:** identificare e correggere le inesattezze nei dati. CiÃ² puÃ² comportare la gestione dei valori mancanti, la rimozione dei duplicati o la risoluzione delle incongruenze.
- **Convalida dello schema:** assicurarsi che i dati in arrivo siano conformi allo schema predefinito in termini di struttura, tipi e vincoli.
- **Controllo di qualitÃ :** verificare lâ€™integritÃ , la qualitÃ  e la completezza dei dati.

> ðŸ’¡ **Nota:** la convalida puÃ² essere costosa e non sempre applicabile, soprattutto in sistemi di ingestione in tempo reale.

## Trasformazione dei dati

- **Arricchimento dei dati:** combinare dati provenienti da piÃ¹ sorgenti o aggiungere informazioni contestuali per aumentarne il valore informativo.
- **Normalizzazione e aggregazione:** uniformare i formati e aggregare i dati per semplificarne lâ€™analisi e il caricamento nei sistemi di destinazione.
- **Derivazione e calcolo:** creare nuovi attributi o metriche a partire da dati esistenti (es. calcolo di indici o rapporti).
- **Filtraggio:** eliminare i dati indesiderati o irrilevanti.

> ðŸ’¡ **Nota:** la trasformazione Ã¨ un passaggio facoltativo. Lâ€™idea originale dello schema in lettura implica dati grezzi non filtrati; tuttavia, ciÃ² richiede spazio aggiuntivo e non sempre porta valore.

## Caricamento dei dati (destinazione - sink)

Una volta **definito il metodo di acquisizione** â€” che sia in *batch* o in *tempo reale* â€” si procede con lâ€™**avvio del processo di trasferimento dei dati**.
In questa fase, le informazioni vengono inviate verso le **destinazioni previste**, che possono comprendere **database**, **data warehouse** o **data lake**, a seconda dellâ€™architettura del sistema.
Il **caricamento** puÃ² avvenire in modalitÃ  **incrementale**, aggiornando solo i nuovi dati, oppure in modalitÃ  **completa**, sostituendo lâ€™intero insieme di informazioni.
Ãˆ in questo momento che i dati vengono **effettivamente ingeriti nel sistema**, diventando disponibili per lâ€™elaborazione e lâ€™analisi.

## Monitoraggio e registrazione

Durante il processo di acquisizione Ã¨ fondamentale **implementare meccanismi di gestione degli errori**, in modo da poter **intercettare e gestire eventuali anomalie** o **ripetere automaticamente i tentativi di inserimento** in caso di fallimento.
Allo stesso tempo, Ã¨ importante predisporre un sistema di **registrazione (logging)** che consenta di **tracciare ogni fase del processo**, includendo eventuali **problemi di connessione alle fonti dati**, **errori di parsing** o **violazioni dello schema**.
Queste pratiche garantiscono **maggiore affidabilitÃ , tracciabilitÃ  e trasparenza** nellâ€™intero flusso di data ingestion.

## Automazione e programmazione

- **Automazione delle pipeline:** utilizzare strumenti e piattaforme per automatizzare l'inserimento dei dati.
- **Pianificazione:** programmare attivitÃ  di acquisizione periodiche per i processi batch.

Spesso questi strumenti agiscono come un **agente** connesso alla generazione dei dati.

## Sicurezza e conformitÃ 

Un aspetto essenziale del processo di acquisizione riguarda la **sicurezza dei dati**.
I **dati sensibili** devono essere **crittografati** sia **durante la trasmissione** sia **a riposo**, per proteggerli da accessi non autorizzati o intercettazioni.
Ãˆ inoltre necessario **implementare rigorosi controlli di accesso**, assicurando che solo gli **utenti autorizzati** possano acquisire, modificare o visualizzare le informazioni.
Infine, lâ€™intero processo deve essere conforme alle principali **normative sulla protezione dei dati**, come il **GDPR**, lâ€™**HIPAA** o altre **regolamentazioni specifiche del settore**, garantendo cosÃ¬ la piena tutela della privacy e la sicurezza delle informazioni gestite.
